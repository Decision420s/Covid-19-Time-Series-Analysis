---
title: "STAT 153 Project"
author: "Yewen Zhou"
date: "12/12/2020"
output: 
        bookdown::pdf_document2:
                fig_caption: yes
                toc: false
header-includes:
- \usepackage{float}
- \usepackage{graphicx}
- \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
```{r, include=FALSE}
library("astsa")
library("forecast")
library("knitr")
library("kableExtra")
library("FitAR")
`LBQPlot` <- function(res, lag.max=30, StartLag=k+1, k=0, SquaredQ=FALSE, title){
stopifnot(k>=0, lag.max-StartLag>0, length(res)>lag.max)
ans<-LjungBoxTest(res, k=k, lag.max=lag.max, StartLag=StartLag, SquaredQ=FALSE)
m <- ans[,"m"]
pv <- ans[,"pvalue"]
plot(m, pv, xlab="Lag", ylab="p-value", ylim=c(0,1), main=title)
abline(h=0.05, col="red", lty=2)
}
```
# Executive Summary

The number of new cases of COVID is expected to follow a cyclic pattern in the next 10 days with a drop around day 64, a rise around day 66, and a drop around day 70. According to our SARIMA(q=1,d=1,D=1,Q=1,S=7) model, the lowest point is expected to occur on day 64 with around 565 new cases, and the highest point is expected to occur on day 66 with around 1130 new cases. The model suggests that the situation of COVID does not look promising as the trend will continue to increase and necessary measures should be taken in the Gotham city to slow down the spread of the virus.

# Exploratory Data Analysis

In the COVID data, marked on the left in Figure \ref{fig:EDA}, the number of new cases appears to have a quadratic relationship with the time. From the periodogram, COVID data is plotted on the frequency domain over index j and it is shown that there are seasonal components at frequencies of 1/60, 8/60, and 9/60. Since the corresponding periods 60/8 and 60/9 are not integers, this might happen due to leakage. Additionally, there appears to be heteroskedasticity in the data. The standard deviation appears to be increasing linearly with the mean, which is implying that the variance appears to be increasing quadratically with the mean.

```{r EDA, echo=FALSE, fig.cap="COVID New Cases and Periodogram.", fig.align = 'center', fig.height=3.5, fig.width = 8, out.width = '90%'}
covid <- read.csv('projectdata_covid.csv')
# plot 
days <- time(covid$Date)
layout(matrix(c(1,2),ncol=2))
plot(days, covid$New.Cases, type="l", ylab="New Cases", xlab="Days", main="COVID data")
pgram <- function(x, title){
  m <- floor(length(x)/2)
  pgram <- abs(fft(x)[2:(m+1)])^2/length(x)
  plot(pgram, type = "h", xlab="Index", ylab="", main=title)
  abline(h=0)
}
pgram(covid$New.Cases, "Periodogram of the COVID data")
```

# Models Considered

To model the natural signal in this data, both a parametric model and a differencing approach are used. The remaining stationary noise are addressed with ARMA models in the following sections.

## Parametric Signal Model

First, the heteroskedasticity is addressed by applying a log VST (Variance Stabalizing Tranform) on the COVID data. Then, the spectral density of the transformed data is plotted, which shows seasonal components at frequencies of 5/60, 8/60, 9/60, and 17/60. These information leads to a parametric signal model with quadratic trend and sinusoids at frequencies as above. This deterministic model is detailed in the equation below, where $log(\\text{COVID}_t)$ is the logarithemic transform of the COVID data and $X_t$ is the additive noise term.

$$
\begin{aligned}
log(\text{COVID}_t) &= \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 cos(\frac{10 \pi t}{60}) + \beta_4 sin(\frac{10 \pi t}{60}) + \beta_5 cos(\frac{16 \pi t}{60}) \\
&+ \beta_6 sin(\frac{16 \pi t}{60}) + \beta_7 cos(\frac{18 \pi t}{60}) + \beta_8 sin(\frac{18 \pi t}{60}) + \beta_9 cos(\frac{34 \pi t}{60}) + \beta_{10} sin(\frac{34 \pi t}{60}) + X_t \notag
(\#eq:param-equation)
\end{aligned}
$$

Figure \ref{fig:signal-1} presents relevant plots including the spectral density of transformed data, the fitted model, residuals, and the spectral density of residuals.

```{r signal-1, echo=FALSE, fig.cap='Spectral Density of $log(\\text{COVID}_t)$, Fitted Model, Residuals, and Spectral Density of Residuals. $log(\\text{COVID}_t)$ is the logarithemic tranform of the COVID data.', fig.align='center', fig.pos="H", out.width="80%"}
# first use a log transform
logY <- log(covid$New.Cases)
# speculate possible frequency using pgram 
# look at pgram and select frequency 
# plot a two-by-two diagram 
par(mfrow=c(2, 2))
mvspec(logY, main="Spectral Density of log(COVID)", xlab="Frequency", ylab="")
abline(v=c(5/60, 8/60, 9/60, 17/60), col="red")
n <- nrow(covid)
cos1 <- cos(2*pi*days*5/n); sin1 <- sin(2*pi*days*5/n)
cos8 <- cos(2*pi*days*8/n); sin8 <- sin(2*pi*days*8/n)
cos9 <- cos(2*pi*days*9/n); sin9 <- sin(2*pi*days*9/n)
cos17 <- cos(2*pi*days*17/n); sin17 <- sin(2*pi*days*17/n)
model1 <- lm(logY ~ days + I(days^2) + cos1 + sin1 + cos8 + sin8 + cos9 + sin9 + cos17 + sin17)
plot(days, logY, type = "l", xlab="Days", ylab = "COVID Cases in log Scale", main="Fitted Model")
# plot the fitted data 
lines(days,model1$fitted.values,lwd=2,col="green")
# plot the residuals 
plot(days, model1$residuals, type="l", xlab="Days", ylab="Residuals", main="Residuals of Parametric Model")
# inspect residuals to see if there are any other frequency left
mvspec(model1$residuals, main="Spectral Density of Residuals", xlab="Frequency", ylab="")
```

It can be seen from the plots that the model fits the transformed data well. Furthermore, the residuals appears to be stationary with a constant mean around 0 and a constant variance. The spectral density plot of residuals looks to be distributed evenly across the frequency domain, which is a good sign that major frequencies have been captured and implies that the residuals might be white noise.

### Parametric signal + ARMA(0,0)
The ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) plots for the parametric model's residuals are shown in Figure \ref{fig:acf-pacf-signal-1}. Because there are no lags with ACF/PACF magnitudes beyond the 95% confidence bands, the residuals is likely to be white noise. Thus, ARMA(0,0) is proposed, and this model implies the ACF and PACF indicated by the blue circles in Figure \ref{fig:acf-pacf-signal-1}, which fit the general pattern of the sample autocorrelations. The Ljung-Box Test of this model, marked on the bottom left in Figure \ref{fig:acf-pacf-signal-1}, shows that all p-values are above the rejection threshold, which implies that residuals is white noise and the model is a good fit.
```{r, echo=FALSE, fig.show="hide", results="hide"}
residuals1 <- model1$residuals
# model 1.1 is parametric signal + ARMA(0,0)
model1_1 <- sarima(residuals1, p=0, d=0, q=0, P=0, D=0, Q=0)
model1_2 <- sarima(residuals1, p=0, d=0, q=1, P=0, D=0, Q=0)
```


```{r acf-pacf-signal-1, echo=FALSE, results="hide", fig.cap="Autocorrelation function (ACF), partial autocorrelation function (PACF) values for the parametric signal model's residuals, and Ljung-Box Test of residuals for ARMA(0,0) and ARMA(0,1). Blue circles reflect the ARMA(0,0) model, while the orange circles reflect the ARMA(0,1) model.", fig.align = 'center', fig.pos="H", fig.height=6, fig.width = 8, out.width="80%"}
par(mfrow = c(2, 2))
lags <- 25
acf(model1$residuals, main="ACF of Residuals", lag.max=lags)
model1_1_acf <- ARMAacf(ar=0, ma=0, lag.max=lags)
model1_2_acf <- ARMAacf(ar=0, ma=model1_2$fit$coef[1], lag.max=lags)
points(0:lags, model1_1_acf, col="blue")
points(0:lags, model1_2_acf, col="orange")
pacf(model1$residuals, main="PACF of Residuals", lag.max=25)
model1_1_pacf <- ARMAacf(ar=0, ma=0, lag.max=lags, pacf = TRUE)
model1_2_pacf <- ARMAacf(ar=0, ma=model1_2$fit$coef[1], lag.max=lags, pacf = TRUE)
points(1:lags, model1_1_pacf, col="blue")
points(1:lags, model1_2_pacf, col="orange")
LBQPlot(model1_1$fit$residuals, title = "Ljung-Box Test of Residuals for ARMA(0,0)")
LBQPlot(model1_2$fit$residuals, title = "Ljung-Box Test of Residuals for ARMA(0,1)")
```

### Parametric signal + ARMA(0,1)
Since there is no evidence for anything other than the white noise, the second model is selected as MA(1) with its theoretical ACF/PACF values shown as orange circles in Figure \ref{fig:acf-pacf-signal-1}. This model captures the magnitude of ACF at lag=1 and the magnitude of PACF at lag=2 better than the first suggested model. The Ljung-Box Test of this model, marked on the bottom right in Figure \ref{fig:acf-pacf-signal-1} shows that all p-values are above the rejection threshold, which suggests that the residuals after fitting ARMA(0,1) is likely to be white noise. Moreover, the p-values are higher than those in the first suggested model. These observations imples that ARMA(0,1) might be a better fit.

## Differencing
In addition to fitting a parametric model on the signal, differencing is also considered. The periodogram of the log transformed data shows seasonal components at frequencies of 8/60 and 9/60, which correspond to periods at 60/8=7.5 and 60/9=6.67. This might occur due to leakage where the true period is 7, which is essentially weekly effect and makes more sense. Furthermore, to remove the quadratic trend in the tranformed data, a second differencing with lag=1 is applied after the first differencing at lag=7. Thus, the differencing procedure is shown in the equation below, where $\text{COVID}_t$ represents the COVID time series data, $X_t$ represents the log transformed data, and $Y_t$ represents the filtered data after differencing.

$$
\begin{aligned}
X_t &= log(\text{COVID}_t)\\
Y_t &= \nabla_7 \nabla X_t\\
&= X_t - X_{t-1} - X_{t-7} + X_{t-8}\\
\end{aligned}
$$
```{r signal-2, echo=FALSE, results='hide', fig.cap='Periodogram of $log(\\text{COVID}_t)$ and Residuals after differnecing. $log(\\text{COVID}_t)$ is the logarithemic transform of the COVID data.',  fig.align = 'center', fig.pos="H", fig.height=3, fig.width = 8, out.width="80%"}
layout(matrix(c(1,2),ncol=2))
pgram(logY, "Periodogram of Log(Y)")
sea_seven = diff(diff(logY, lag=7))
plot(sea_seven, type="l", xlab="Days", ylab="COVID Cases in log Scale", main="Residuals")
# plot the periodogram on the data 
```
From Figure \ref{fig:signal-2}, we see that after differencing, the data appears to be stationary with a constant mean around 0 and a constant variance. Although there appears to be seasonal components in the filtered data, it is not deterministic. In the following sections, two MSARMA models are proposed to address the stationary residuals.


### Differencing + MSARMA(0,1)x(0,1)[7]
The ACF and PACF for the filtered data are shown in Figure \ref{fig:acf-pacf-signal-2}.


```{r, echo=FALSE, fig.show = 'hide', results='hide', fig.align = 'center', fig.pos="H"}
# auto.arima(sea_seven)
model2_1 <- sarima(sea_seven, p=0,d=0,q=1,P=0,D=0,Q=1,S=7)
model2_2 <- sarima(sea_seven, p=1,d=0,q=0,P=1,D=0,Q=0,S=7)
```

```{r acf-pacf-signal-2, echo=FALSE, results="hide", fig.cap="ACF, PACF of residuals, Ljung-Box Test of residuals for MSARMA(0,1)x(0,1)[7] and MSARMA(1,0)x(1,0)[7].", fig.align='center', fig.pos='H', out.width="80%", fig.height=5, fig.width = 8}
# layout(matrix(c(1, 2), ncol=2))
par(mfrow = c(2, 2))
lags <- 25
acf(sea_seven, main="ACF of Residuals", lag.max = lags)
model2_1_acf <- ARMAacf(ma=c(model2_1$fit$coef[1], rep(0, 5),
                              model2_1$fit$coef[2],
                             model2_1$fit$coef[1]*model2_1$fit$coef[2]), lag.max = lags)
model2_2_acf <- ARMAacf(ar=c(model2_2$fit$coef[1], rep(0, 5),
                              model2_2$fit$coef[2],
                             model2_2$fit$coef[1]*model2_1$fit$coef[2]), lag.max = lags)
points(0:lags, model2_1_acf, col="blue")
points(0:lags, model2_2_acf, col="orange")
pacf(sea_seven, main="PACF of Residuals", lag.max = lags)
model2_1_pacf <- ARMAacf(ma=c(model2_1$fit$coef[1], rep(0, 5),
                              model2_1$fit$coef[2],
                             model2_1$fit$coef[1]*model2_1$fit$coef[2]),
                         lag.max = lags, pacf = TRUE)
model2_2_pacf <- ARMAacf(ar=c(model2_2$fit$coef[1], rep(0, 5),
                              model2_2$fit$coef[2],
                             model2_2$fit$coef[1]*model2_2$fit$coef[2]),
                         lag.max = lags, pacf = TRUE)
points(1:lags, model2_1_pacf, col="blue")
points(1:lags, model2_2_pacf, col="orange")
LBQPlot(model2_1$fit$residuals, title = "LB Test of Residuals for MSARMA(0,1)x(0,1)[7]")
LBQPlot(model2_2$fit$residuals, title = "LB Test of Residuals for MSARMA(1,0)x(1,0)[7]")
```
The plots show that the magnitudes of ACF and PACF beyond the 95% confidence band occur at lag=1 and lag=7. The former magnitude at lag=1 suggests a possible ARMA model with q=1, the latter magnitude at lag=7 suggests a possible seasonal MA component with S=7 and Q=1. Based on the above observations, a multiplicative seasonal ARMA model, MSARMA(0,1)$\times$(0,1)[7] is proposed. The theoretical ACF and PACF values are plotted as blue circles in Figure \ref{fig:acf-pacf-signal-2}. These circles show that this model captures the general pattern of ACF and PACF well especially at lag=1 and lag=7. The Ljung-Box Test of the residuals is shown on the bottom left in Figure \ref{fig:acf-pacf-signal-2}. It shows all p-values are not significant, which suggests that the residuals is white noise and the model is a good fit.


### Differencing with MSARMA(1,0)x(1,0)[7]
To address the magnitude in PACF at lag=7, a possible seasonal AR component is also considered with P=1 and S=7. To address the magnitude in PACF at lag=1, p is chosen to be 1. Thus, the alternative model, MSARMA(1,0)$\times$(1,0)[7] is proposed. The fit of this model is shown as orange circles in Figure \ref{fig:acf-pacf-signal-2}. Clearly, this model failed to capture the general pattern and didn't fit sample autocorrelations as well as the first suggested model. The Ljung-Box Test for this model is plotted on the bottom right in Figure \ref{fig:acf-pacf-signal-2}. Although all p-values are not significant, they are lower than the values in the first suggested model. Based on these observations, MSARMA(1,0)x(1,0)[7] is not as a good fit as MSARMA(0,1)x(0,1)[7].


# Model Comparison and Selection
```{r, fig.show='hide', results="hide"}
data <- logY
sum_squared_errors <- c(model1=0, model2=0, model3=0, model4=0)
for (i in 40:50) {
        # split the training set and the test set 
        train <- window(data, end=i)
        test <- window(data, start=i+0.01, end=i+10)
        n <- 60
        cos1 <- cos(2*pi*days[1:i]*1/n); sin1 <- sin(2*pi*days[1:i]*1/n)
        cos8 <- cos(2*pi*days[1:i]*8/n); sin8 <- sin(2*pi*days[1:i]*8/n)
        cos9 <- cos(2*pi*days[1:i]*9/n); sin9 <- sin(2*pi*days[1:i]*9/n)
        cos17 <- cos(2*pi*days[1:i]*17/n); sin17 <- sin(2*pi*days[1:i]*17/n)
        cos1_t <- cos(2*pi*days[(i+0.01):(i+10)]*1/n)
        sin1_t <- sin(2*pi*days[(i+0.01):(i+10)]*1/n)
        cos8_t <- cos(2*pi*days[(i+0.01):(i+10)]*8/n)
        sin8_t <- sin(2*pi*days[(i+0.01):(i+10)]*8/n)
        cos9_t <- cos(2*pi*days[(i+0.01):(i+10)]*9/n)
        sin9_t <- sin(2*pi*days[(i+0.01):(i+10)]*9/n)
        cos17_t <- cos(2*pi*days[(i+0.01):(i+10)]*17/n)
        sin17_t <- sin(2*pi*days[(i+0.01):(i+10)]*17/n)
        model_train <- lm(train ~ days[1:i] + cos1 + sin1 + cos8 + sin8 + cos9 + sin9
                + cos17 + sin17)
        test_matrix <- model.matrix(~days[(i+0.001):(i+10)]
                                   + cos1_t + sin1_t + cos8_t + sin8_t + sin9_t + cos9_t
                + cos17_t + sin17_t)
        # Parametric Signal + ARMA(0,0)
        forecast1_1 <- test_matrix %*% model_train$coefficients +
          sarima.for(model_train$residuals, n.ahead=10, p=0,d=0,q=0,P=0,D=0,Q=0,S=0)$pred
        # Parametric Signal + ARMA(0,1)
        forecast1_2 <- test_matrix %*% model_train$coefficients +
          sarima.for(model_train$residuals, n.ahead=10, p=0,d=0,q=1,P=0,D=0,Q=0,S=0)$pred
        # Differencing + MSARMA(0,1)x(0,1)[7]
        forecast2_1 <- sarima.for(train, n.ahead=10, p=0,d=1,q=1,P=0,D=1,Q=1,S=7)$pred
        # Differencing + MSARMA(1,0)x(1,0)[7]
        forecast2_2 <- sarima.for(train, n.ahead=10, p=1,d=1,q=0,P=1, D=1,Q=0,S=7)$pred
        # accumulat errors 
        sum_squared_errors[1] <- sum_squared_errors[1] + sum((forecast1_1-test)^2)
        sum_squared_errors[2] <- sum_squared_errors[2] + sum((forecast1_2-test)^2)
        sum_squared_errors[3] <- sum_squared_errors[3] + sum((forecast2_1-test)^2)
        sum_squared_errors[4] <- sum_squared_errors[4] + sum((forecast2_2-test)^2)
}
```

Using cross-validation that rolls through data from time stamp=40 to 50, each time forecasting the data in the next 10 days with root-mean-square prediction error, RMSPE, we have the following table \ref{tab:rmsetable}. It shows that the RMSPE of the SARIMA(q=1,d=1,D=1,Q=1,S=7) model, which is the same model proposed in section 3.2.1, is the best overall according to this cross-validation exercise, and therefore this model will be used for forecasting.
```{r rmsetable, fig.pos = 'H', fig.align = 'center'}
#RMSE table
rmse <- matrix(sqrt(sum_squared_errors/10), nrow=4,ncol = 1)
colnames(rmse) <- "RMSPE"
rownames(rmse) <- c(
        "Parametric Model + ARMA(0,0)",
        "Parametric Model + ARMA(0,1)",
        "SARIMA(q=1,d=1,D=1,Q=1,S=7)",
        "SARIMA(p=1,d=1,D=1,P=1,S=7)"
        )
rmse <- kable(rmse,caption = "Cross-validated out-of-sample root mean squared prediction error for the four models under consideration.")
kable_styling(rmse, position = 'center', latex_options = 'HOLD_position')
```

# Results
The SARIMA(q=1,d=1,D=1,Q=1,S=7) model will be used to forecast COVID new cases in the next 10 days, and the detailed equation is listed below where $\text{COVID}_t$ represents the number of new cases of COVID on day $t$, $X_t$ represents the log transformed data, $Y_t$ represents the filtered data after differencing, and $W_t$ represents the white noise process with variance $\sigma_W^2$.
$$
\begin{aligned}
X_t &= log(\text{COVID}_t)\\
X_t &= X_{t-1} + X_{t-7} - X_{t-8} + W_t + \theta W_{t-1} + \Theta W_{t-7} + \Theta \theta W_{t-8}\\
\end{aligned}
$$
$\theta$, $\Theta$ are coefficients which will be estimated in the next subsection.


## Estimation of model parameters
Estimates of the model parameters, the standard errors, and parameter descriptions are given in Table \ref{tab:estimatetable}. One interesting thing to notice is that all values are pretty small. This might happen due to the fact that the COVID data is first transformed into the log scale.
```{r estimatetable}
x <- matrix(nrow = 3, ncol = 3)
colnames(x) <- c("Estimate", "SE", "Coefficient Description")
rownames(x) <- c("$\\theta$", "$\\Theta$", "$\\sigma_M^2$")
x[3,1] <- round(model2_1$fit$sigma2, 4)
x[3,2] <- ""
x[,3] <- c("MA coefficient", "Seasonal MA coefficient", "Variance of White Noise")
x[1:2,1:2] <- model2_1$ttable[1:2,1:2]
x <- kable(x, caption = 'Model Estimates', escape = FALSE)
kable_styling(x, position = 'center', latex_options = 'HOLD_position')
```
## Prediction
Figure \ref{fig:final-pred} shows the forecast values of COVID new cases in next 10 days in log scale and the original scale. The model predicts that the new cases will follow a cyclic pattern with a drop up to day 64, a rise up to day 66, then a drop up to day 70. According to the forecasts, the lowest point is expected to occur on day 64 with around 565 new cases, and the highest point is expected to occur on day 66 with around 1130 new cases. The trend will be slowly going upwards, which suggests that necessary measures should be taken in the Gotham city to help slow down the spread of the virus.


```{r, echo = FALSE, results = 'hide', fig.show = 'hide', fig.cap = 'Forecasts of COVID new cases', fig.pos = 'H', fig.width = 8, fig.height = 4, out.width = '90%'}
sarima.for(covid$New.Cases, n.ahead = 10, p=0,d=1,q=1,P=0,D=1,Q=1,S=7)$pred
```

```{r prediction, echo = FALSE, results="hide", fig.show = "hide"}
#prediction table
prediction <- matrix(nrow=2,ncol = 10)
prediction[1,] <- exp(1)^sarima.for(xdata=logY, n.ahead=10, p=0,d=1,q=1,P=0,D=1,Q=1,S=7)$pred
prediction[2,] <- exp(1)^sarima.for(xdata=logY, n.ahead=10, p=0,d=1,q=1,P=0,D=1,Q=1,S=7)$se
colnames(prediction) <- c("61", "62", "63", "64", "65", "66", "67", "68", "69", "70")
rownames(prediction) <- c("Prediction", "Standard Error")
```


```{r final-pred, echo = FALSE, results = 'hide', fig.cap = 'Forecasts of COVID new cases in next 10 days in log scale and original scale.', fig.align = 'center', fig.pos = 'H', fig.height = 6, fig.width = 8, out.width = '90%'}
par(mfrow = c(2,1))
sarima.for(logY, n.ahead = 10, p=0,d=1,q=1,P=0,D=1,Q=1,S=7)$pred
plot(covid$New.Cases, xlab="Days", ylab="New Cases", type="o", xlim=c(0,70), lwd = 2)
lines(x=61:70, y=prediction[1,], type='o', col="blue", lwd = 2)
grid(lwd=2)
# lines(x=61:70, y=prediction[1,]+2*prediction[2,], type="l", col="orange", lwd = 2)
# lines(x=61:70, y=prediction[1,]-2*prediction[2,], type="l", col="yellow", lwd = 2)
```
```{r, echo = FALSE, results = 'hide'}
write.table(prediction[1,], file = 'covid_3034360697.csv', sep = ",", row.names = FALSE, col.names =FALSE)
```
